---
title: "Dada2 tutorial"
output: 
  github_document:
    toc: true
    toc_death: 2
---
# Appel des séquences
```{r}
library("dada2")
```
On a appellé le package dada2

Les données ont été téléchargées
On definit la variable Path pour changer d'endroit dans l'arborescense du fichier une fois dézipé MiSeq_SOP :
```{r}
path <- "~/MiSeq_SOP" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```
On voit les fichiers fastq

On crée des variables pour l'analyse du plot, les Read 1 vont dans la variable fnFs et les Read2 dans la variable fnRs

- la foction sample.names : extrait les noms des échantillons, tout en supposant que les noms de fichiers ont un format : NOM DE L'ÉCHANTILLON_XXX.fastq


```{r}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

On a récupéré seulement les read issu d'Illumina --> on va évaluer la qualité de chaque read
On affiche les profils de qualité des Read1 (R1) (= Forward) en avant du premier et du deuxième read issus de la variable crée précédémment FnFs
```{r}
plotQualityProfile(fnFs[1:2])
```

On affiche les profil de qualité des Read2 (R2) (= Reverse) en arrière du premier et du deuxième read issus de la variable crée précédémment FnRs
```{r}
plotQualityProfile(fnRs[1:2])
```
# Filtration

On va filtrer les variables contenant les read 1 et 2. À partir de ces filtrages on attribue des variable et des noms associées 
filtFs : pour les read1
filtRs : pour les read2
```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Les paramètres de filtrage standards : 
-maxN=0 (Dada2 ne nécessite pas de Ns), 
-truncQ=2 : on va tronquer pour les reads avant a 240 et 160 pour les reads arrières
-rm.phix=TRUE 
-maxEE=2  -> le nombre maximum d'"erreurs attendues" autorisées dans une lecture, ce qui est un meilleur filtre que la simple moyenne des scores de qualité
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```
On obtient le nombre de bases (les paires de base) qui sont gardées une fois la filtration effectuées ici que pour les R1. Pour le reads F3D30 S188 R1  001 :7793 nt pour le read avant filtration et 7113 nt après filtration. 680 nt ont été enlevé : les primers ont été supprimés et les nucléotides pas bons. 

## Apprentissage des erreurs 

Il est possible d'avoir des erreurs, avec Dada2 on inspecte les séquences. 
On utilise un modèle d'erreur paramétrique err pour les R1 et R2 

Le But : Le modèle d'erreur de DADA2  permet identifier les positions avec une forte probabilité d'erreur et par la suite changer avec la base la plus probable. Cela veut dire celle quiest présente dans la séquence la plus abondante

On crée les variables : 
-errF : recoit le modèle d'erreur paramétrique par la fonction LearnErrors pour les R1 et R2 filtrés 

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
```

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```

## Visualisation des modèles d'ereur du forward

En abscisse on a la probabilité des mutations et en ordonnée le q score
Q30 : la probabilité que la base trouver sois la bonne 
```{r}
plotErrors(errF, nominalQ=TRUE)
```

Chaque mutation est possible, les taux d'erreur sont indiqués. 
- points  : les taux d'erreur observés pour chaque score de qualité. 
- ligne noire: taux d'erreur estimé après convergence de l'algorithme d'apprentissage. 
- ligne rouge: taux d'erreur attendu selon le Q-score.

Les taux d'erreur estimés correspondent aux taux observés, et les taux d'erreur diminuent avec l'augmentation de la qualité. 

## Exemple d'interférences

```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```
Calcul pour les séquences composées de séquences uniques 
Dans notre exemple avec les donnée de MiSeq_SOP, l'échantillon 1 a 7113 read dont 1979 read unique les autres peuvent être retrouvées plusieus fois. Dans les information supplémentaires on a les redondances, le clustering ... 

On fait pareil pour les reverses (Rs)
```{r}
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

Ici, on inspecte la premiere étagère du premeir placard de dada2
```{r}
dadaFs[[1]]
```

On inspecte la deuxième étagère du premier placard de dada2 (deuxième échantillon)
```{r}
dadaFs[[2]]
```
Le filtrage des reads est fini. 

# Aligner les R1 et R2 en un contig

Le but est de créer des contigs à partir des R1(forwards) et des R2 (reverse)
Ici c'est posssible car c'est une amplification de la région V4 de l'ARN 16S il y a donc un overlap avec read1 et read2 

- verbose : montrer les étapes avec du texte pendant qu'elles sont réalisées
- objet merge: liste de data.frames de chaque échantillon. Chaque data.frame contient la séquence fusionnée, son abondance, et les indices des variantes de la séquence avant et arrière fusionnées.
- mergePairs: suppression des read apparies qui se chevauchaient -> réduction du bruit
- head merge : regarder la première ligne
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

# Construction d'une table d'observation

Table de variant de séquences d'amplicon (ASV), une version à plus haute résolution de la table OTU produite par les méthodes traditionnelles.

On va partir de l'abondance, on crée la table à partir de notre objet merger

- objet seqtab : une table avec en ligne le nombre échantillon, en colonne les séquences elle-même à l'intérieur ou on observe la séquence dans l'échantillon 
- dim : récupérer ou définir la dimension d'un objet.
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```
20 entrées = contigs et 293 colones = la séquence nucléotidique

Une matrice avec des lignes correspondant aux échantillons et des colonnes correspondant aux variantes de séquences. Ce tableau contient 293 ASV, et les longueurs de nos séquences fusionnées se situent toutes dans la plage prévue pour cet amplicon V4.

# Inspection des longueurs de séquences
```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
# Chimères

Pendant  l'amplification pendant PCR, on amplifie avec les primers 
Avec le primer F(pour les forwards), on crée la séquence complémentaire, mais pour x ou y, l'élongation s'arrête 
Comment se créer les chimères : On a le 16S qui restera intact et un autre fragment simple brin plus court. Lors du cycle suivant en PCR, on va avoir un des fragment 16S (rouge par exemple) qui vont s'hydrider sur un autre fragment 16S comme un primer et donc continuer élongation (verte par exemple) pour donner une séquence hybride à la fin qui sera le début du rouge et a la fin du vert

Ce phénomène est rare, mais lorsqu'il y a plein de séquence possible et il faut les enlever du jeu de données 
Il n'est pas possible de les détecter au niveau de la taille de la séquence. En revanche, on peut regarder toutes les séquences rares dont le début correspond à une séquence parent dans ce jeu donnée et la fin d'une autre séquence parent

Appliquer à seqtab et transférer à une nouvelle variable seqtab.nochim

-> 1/5, les 293 uniques mais qui représente plus de séquence dans le jeu de donnée

## Enlever les chimères par méthode consensus 

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
61 chimères obtenu dans les 983 ASV

## Faire le ratio

```{r}
sum(seqtab.nochim)/sum(seqtab)
```

Il y a 0,96% de séquences chimériques dans notre jeu de données

# Construction d'une table 

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```


```{bash}
cd $HOME
wget https://zenodo.org/record/3986799/files/silva_nr99_v138_train_set.fa.gz
```

#Assignation taxonomique

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```

## Assignation taxonomique n°2 Silva species assignement
```{bash}
wget https://zenodo.org/record/3986799/files/silva_species_assignment_v138.fa.gz
```

```{r}
taxa <- addSpecies(taxa, "~/silva_species_assignment_v138.fa.gz")
```
```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```
On a construit une table qui commence au niveau du reigne et qui va jusqu'à l'espèce. Les résultats montrent que les séquences n'ont pas pu être assignées jusqu'à l'espèce voire même au niveua du genre hormispour la séquence 5. 

## Evaluer la précision de l'assignation taxonomique 
```{r}
unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
```

```{r}
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")
```

# Conclusion

```{r}
save.image(file="02_Dada2_tutorial_FinalEnv")
```



